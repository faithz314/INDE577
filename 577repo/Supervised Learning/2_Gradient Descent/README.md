Gradient Descent and Linear Regression

Have you ever felt like walking down one of those parabolas drawn in algebra class? Well, you can do so with gradient descent!

Gradient Descent is an optimization algorithm for  finding a local minimum of a differentiable function. This is widely-used and commonly utilized in machine learning to train models. This algorithm is used in the wider context of Linear Regression- a technique used to predict the value of a variable based on another variable (giving an output after being fed a certain input). We do this by using gradient descent, and we utilize minimization of the loss function, which brings us closer to the minima (more on this later). 

The single neuron model of Linear Regression is a perameterized model with weights and biases, similar to the binary classification task accomplished by the Perceptron. The main difference here is that instead of categorical inputs, inputs to the Linear Regression model are within the domain of real numbers and spit out another real number. The goal becomes to predict numeric outputs, instead of categorizing data into different sets.



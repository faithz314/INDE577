#Gradient Descent and Linear Regression

Have you ever felt like walking down one of those parabolas drawn in algebra class? Well, you can do so with gradient descent!

Gradient Descent is an optimization algorithm for  finding a local minimum of a differentiable function. This is widely-used in machine learning to train models. This algorithm is used in the wider context of Linear Regression- a technique used to predict the value of a variable based on another variable (giving an output after being fed a certain input). We do this by using gradient descent, and we utilize minimization of the cost function, which brings us closer to the minima (more on this later). 

The single neuron model of Linear Regression is a parameterized model with weights and biases, similar to the binary classification task accomplished by the Perceptron. The main difference here is that instead of categorical inputs, inputs to the Linear Regression model are within the domain of real numbers and spit out more real numbers. The goal becomes to predict numeric outputs, instead of categorizing data into different sets.

In this notebook, we will continue our analysis on the penguins dataset. Click on the notebook to read a more detailed description of this algorithm and the dataset.

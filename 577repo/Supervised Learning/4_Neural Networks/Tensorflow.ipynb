{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense & Deep Neural Networks\n",
    "\n",
    "We have only covered simple parametric models so far and we've only been studying the single neuron model. Neural networks is literally an expansion of these concepts. Instead of a single nueorn, neural networks contain multiple layers of interconnected neurons. Each hidden layer of neurons receive input from the last layer. The output of the final layer is the final output of the network as a whole. This algorithm is inspired by the human brain, and definitely a bit more realistic than a single neuron model!\n",
    "\n",
    "Neural networks work for both regression and classification tasks. In particular, neural networks are a great strategy for models that aren't linearly separable (and even look a little wonky :)), and it can pretty much learn any dataset (but we may run into the problem of overfitting). Neural networks can also perform dimensionality reduction, meaning we can take a multidimensional matrix and reduce it down to a smaller dimension to work with. \n",
    "\n",
    "# How does a neural network actually work?\n",
    "Each individual node acts as its own single neuron model, composed of weights and biases and an output. This time, however, we are dealing with an entire layer of nodes. For a single neuron, the $z$ output is the same: $z = b+ w_1x_1 + w_2x_2 +...w_nx_n$. This scalar output effectively functions as a new input for the next layer of neurons- a phase called *feed-forward*. We can do this process for each layer until an output is arrived at. Passing a signal through these layers creates a highly composite function that approximates a target function $f$.\n",
    "\n",
    "The Univervsal Approximation Theorem summarizes this pretty well. Essentially, the use of a nonlinear activation function (i.e.: Sigmoid!?) within the neural network can approximate any \"reasonable\" function, given that a network has at least one hidden layer of nonlinear units and a single linear output. Without this component of nonlinearity, the model would be reduced down to linear regression and it would not be able to learn the complex relationships between points. For example, we can use the Sigmoid function, but other common functions include: tanh, RELU, and Maxout. \n",
    "\n",
    "The next big step to the neural network algorithm is the cost function, dubbed *backpropogating*. We find the gradient of the cost function and update with a learning rate, as backpropogation is a method to adjust the weights from the errors found during learning. Errors from the ouput are sent backward through the network to adjust the weights and bisases. This iteration continues until the cost function is minimized, so this is just a large scale version of our single neuron model!\n",
    "\n",
    "\n",
    "Neural networks are extensively powerful, but they also have their own limitations. For one, neural networks are computationally expensive and have been criticized to require too many training samples in order to be suitable for real-world operation. As such, they also tend to be prone to overfitting and require careful parameter tuning. Still, these remain a powerful tool for solving ML problems. \n",
    "\n",
    "\n",
    "Neural networks are used across many industries, including medical image classifications, financial predictions, chemical compound identification, and many,many more. It's a huge element of supervised learning. In this notebook, we will utilize the TensorFlow package to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1483834 , -0.50336653,  0.22717337,  0.15045643,  0.722004  ,\n",
       "         0.12627143, -0.42363054,  0.21585673,  0.7473945 , -0.2706663 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22302099, 0.04275627, 0.08877064, 0.08221509, 0.14560339,\n",
       "        0.08025058, 0.04630509, 0.08777171, 0.14934768, 0.05395855]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5226011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3027 - accuracy: 0.9129\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1449 - accuracy: 0.9577\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1083 - accuracy: 0.9665\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0880 - accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0763 - accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ab03be9df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0687 - accuracy: 0.9780 - 574ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0687047615647316, 0.9779999852180481]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "PCA is the last machine learning technique that we will be talking about in this repository!\n",
    "\n",
    "The goal of PCA is to reduce the dimensionality of the feature vectors used in the algorithm. For example, given a 3d dataset, we make it 2d. Trying to analyze multi-dimensional data becomes very complicated very fast, so PCA is great for simplifying the data. Indeed, PCA reduces such data strategically and tries to retain as much information as possible. \n",
    "\n",
    "PCA tries to find a new set of variables, or principle components, which are linear combinations of the original variables. In linear algebra terms, the principle component is the largest eigenvalue's vector in which data is projected onto. Subsequent principal components are orthogonal to the previous one and try to capture the remaining variance.\n",
    "\n",
    "Here are the steps to PCA:\n",
    "\n",
    "1) Standardizing and centering the data.\n",
    "\n",
    "Given a dataset with n observations and m variables, we can form a matrix X of size n x p (each row is an observation and each column is a variable). PCA will aim to find a matrix Z that maps X onto a new space Y, where Y has lower dimensionality than X. To center the data, each row is averaged: $x- mean$, and the z-score is found: $\\frac{x - mean}{stddev}$\n",
    "\n",
    "2) Computing the covariance matrix of the centered data.\n",
    "\n",
    "The covariance matrix, $S$, is calculated as follows: $S = \\frac{1}{n-1}XX^T$. Covariance is the measure of the total variation between 2 random variables. It is the most holistic measure of variance (in comparison to just, say, normal variance -although that can be seen through the diagonals anyway), so we use it for PCA.\n",
    "\n",
    "3) Finding the eigenvalues and eigenvectors of $S$.\n",
    "\n",
    "The eigenvectors of S are denoted $w1, w2, ...wn$ and the eigenvalues are $\\lambda_1, \\lambda_2,....\\lambda_n$ We solve this optimization problem with Singular Value Decomposition, which decomposes X into: $X= \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}$. U is a n x m matrix of left singular vectors, $\\Sigma$ is a m x m diagonal matrix of singular values, and V is a n x n matrix of right singular vectors. \n",
    "\n",
    "4) Finding the principal components and reducing the dimensionality.\n",
    "\n",
    "If we arrange the eigenvalues in decreasing order, the first k principal components directions' are given by the first eigenvector, then the second eigenvector, and so on (so the highest eigenvalues are chosen first). The principal components matrix thus produces this result: $Y = XZ$, where Z is the transformation matrix that holds the eigenvalue weights.  Matrix X is multiplied by Z and its projected onto Y, which is a lower dimension matrix.\n",
    "\n",
    "![pca in a nutshell](https://media.licdn.com/dms/image/C4D12AQF2ZVi92nypDw/article-cover_image-shrink_720_1280/0/1520126718800?e=2147483647&v=beta&t=0WHgQqbRkQi6RpjIWTX9U8-lb00uG85tiPeFZ9L37hM)\n",
    "\n",
    "PCA is really good for noise reduction and data visualization, as it simplifies the data into a more manageable size. It's a versatile tool used in many tools, including image processing, pattern recognition, and data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Fashion-MNIST Source](https://www.kaggle.com/datasets/zalando-research/fashionmnist)\n",
    "\n",
    "For this notebook, we will perform PCA on the fashion MNIST dataset!! \n",
    "What a fun way to end our machine learning tour!\n",
    "\n",
    "Similar to the MNIST dataset, the fashion-MNIST dataset contains grayscale images of fashion items, instead of handwritten digits. Each image is a 28 x 28 px image of 10 classes:\n",
    "\n",
    "0) T-shirt/top\n",
    "1) Trouser\n",
    "2) Pullover\n",
    "3) Dress\n",
    "4) Coat\n",
    "5) Sandal\n",
    "6) Shirt\n",
    "7) Sneaker\n",
    "8) Bag\n",
    "9) Ankle boot\n",
    "\n",
    "Luckily for us, because this is a commonly-used dataset, we can use Tensorflow to import this dataset directly into our repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      0       0       0       0       0       0       0       0       9   \n",
      "1      1       0       0       0       0       0       0       0       0   \n",
      "2      2       0       0       0       0       0       0      14      53   \n",
      "3      2       0       0       0       0       0       0       0       0   \n",
      "4      3       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       8  ...       103        87        56         0         0         0   \n",
      "1       0  ...        34         0         0         0         0         0   \n",
      "2      99  ...         0         0         0         0        63        53   \n",
      "3       0  ...       137       126       140         0       133       224   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2        31         0         0         0  \n",
      "3       222        56         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df= pd.read_csv(r\"C:\\Users\\faith\\inde 577\\INDE577\\577repo\\Supervised Learning\\Datasets\\fashion-mnist_test.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The goal of PCA is to reduce the dimensionality of the feature vectors used in training machine learning algorithms. The steps in PCA are:\n",
    "\n",
    "1. **Stadardize (center and scale) the data.** \n",
    "\n",
    "To center the data, we average each row by replacing the value $x$ by \n",
    "$$\n",
    "x - \\text{mean}\n",
    "$$\n",
    "Data values may have vastly different ranges, and so, to ensure that PCA is not selecting wrong directions in describing data variation, we also divide by the standard deviation. That is, we scale the data in each variable by finding the *$z$-scores*:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\text{mean}}{\\text{standard devation}}\n",
    "$$\n",
    "\n",
    "Finally, we form the $m\\times n$ matrix $A$. \n",
    "\n",
    "2. **Compute the covariance or correlation matrix**:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{n-1}AA^T\n",
    "$$\n",
    "\n",
    "If we are working with only centered data, the above matrix is the covariance matrix, and if we are working with scaled data, then $S$ is the correlation matrix. The entries on the diagonal are the variances (or correlations) for each variable and the off-diagonal entries are the covariances (or correlations) between two variables: positive covariance indicates that the variables are directly related (when one increases, the other increases as well), negative covariance indicates inverse relationship (when one increases, the other decreases). This matrix is symmetric of size $m \\times m$, so its columns are of the same size as the columns of $A$.\n",
    "\n",
    "3. **Find the eigenvalues and the orthonormal eigenvectors of $S$.** \n",
    "\n",
    "These eigenvectors are columns of the matrix $U$ in the singular value decomposition of $A$, up to the factor $n-1$. Further, we denote the eigenvalues by $\\sigma_{i}^{2}$. This is equivalent to the **Singular Value Decomposition** of our shifted training set matrix $A$,\n",
    "\n",
    "$$\n",
    "A = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = X - X.mean(axis = 0)\n",
    "\n",
    "U, sigma, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print(f\"np.shape(U) = {np.shape(U)}\")\n",
    "print(f\"np.shape(sigma) = {np.shape(sigma)}\")\n",
    "print(f\"np.shape(Vt) = {np.shape(Vt)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mat = np.diag(sigma)\n",
    "print(f\"A == U * sigma_mat * Vt: {np.allclose(A, np.dot(U, np.dot(sigma_mat, Vt)))} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "4. **Find the principal components.**\n",
    "\n",
    "We arrange the eigenvalues found in the previous step in the decreasing order. The first principal component $PC_1$ is in the direction of the 1st eigenvector, the second principal component $PC_2$ is in the direction of the 2nd eigenvector, etc. The entries of each $PC_i$ are called *loading scores* and they tell us how the $PC_i$ is a linear combination of features.\n",
    "\n",
    "5. **Reduce the dimension of the data.**\n",
    "\n",
    "We project data points (i.e., columns of $A$) onto the selected principal components (i.e., several eigenvectors of $S$). By the Eckart-Young theorem we know that the line closest to the data points is in the direction of $PC_1$, etc (”closest” is in the sense of perpendicular least squares).\n",
    "\n",
    "In addition, the total variance, which is the trace of $S$, is\n",
    "\n",
    "$$\n",
    "T = \\text{trace}(S) = \\frac{\\sigma_{1}^{2} + \\dots + \\sigma_{m}^{2}}{n-1},\n",
    "$$\n",
    "\n",
    "and the $i$-th principle component $PC_i$ explains\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma_{i}^{2}/(n-1)}{T} = \\frac{\\sigma_{i}^{2}}{\\sigma_{1}^{2} + \\dots + \\sigma_{m}^{2}}\n",
    "$$\n",
    "\n",
    "of the total variation. We use a scree plot to graph the percentages of variation that each $PC_i$ accounts for. Also, the sum of squared distances from the points projected to $PC_i$ to the origin is the eigenvalue for $PC_i$ or the squared singular value $\\sigma_{i}^{2}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To project the data contained in $A$ onto the first two principle component axis, we compute $A [PC_1 PC_2]$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC1 = Vt.T[:, 0]\n",
    "PC2 = Vt.T[:, 1]\n",
    "PC3 = Vt.T[:, 2]\n",
    "PC4 = Vt.T[:, 3]\n",
    "\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = A.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors(y):\n",
    "    if y == \"setosa\":\n",
    "        return \"red\"\n",
    "    elif y == \"versicolor\":\n",
    "        return \"magenta\"\n",
    "    else:\n",
    "        return \"lightseagreen\"\n",
    "\n",
    "c = [colors(label) for label in y]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X2D[:, 0], X2D[:, 1], c = c)\n",
    "plt.xlabel(\"First Principle Component\", fontsize = 15)\n",
    "plt.ylabel(\"Second Principle Component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaled_X = preprocessing.scale(X)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "print(f\"pca.explained_variance_ratio_ = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, 2)\n",
    "print(f\"per_var = {per_var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart for the above array\n",
    "# This chart is called a \"Scree Plot\"\n",
    "\n",
    "labels = [f\"PC{i}\" for i in range(1,5)]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.bar(x = range(1, 5), height = per_var, tick_label = labels)\n",
    "plt.xlabel('Principal Component', fontsize = 15)\n",
    "plt.ylabel('Percentage of Variation', fontsize = 15)\n",
    "plt.title('Scree Plot', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call function pca.components_ to see how each PC is obtained\n",
    "# as a linear combination of the original coordinates\n",
    "\n",
    "# for example, here PC1 = 0.522 * sepal_length + 0.372 * sepal_width - 0.721 * petal_length - 0.262 * petal_width\n",
    "\n",
    "features = list(set(iris.columns) - {\"species\"})\n",
    "\n",
    "pd.DataFrame(data = pca.components_, columns = labels, index = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pca.transform to see how our data looks like in the new coordinate system\n",
    "\n",
    "# this will be used later to plot our data in the first two coordinates PC1, PC2\n",
    "\n",
    "pca_data = pca.transform(scaled_X)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_data, columns = labels)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add to the previous table the labels for each flower\n",
    "\n",
    "projected_df = pd.concat([pca_df, iris.species], axis = 1)\n",
    "projected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_df[\"color\"] = c\n",
    "projected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "colors = [\"red\", \"magenta\", \"lightseagreen\"]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    temp_df = projected_df[projected_df['species'] == target]\n",
    "    plt.scatter(temp_df[\"PC1\"],\n",
    "                temp_df[\"PC2\"],\n",
    "                c = color)\n",
    "    \n",
    "plt.xlabel('PC1', fontsize = 15)\n",
    "plt.ylabel('PC2', fontsize = 15)\n",
    "plt.title('Two-component PCA', fontsize = 18)\n",
    "plt.legend(targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer(as_frame = True)\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cancer[\"feature_names\"]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = cancer[\"target_names\"]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = cancer[\"frame\"]\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_df[features].to_numpy()\n",
    "scaled_X = preprocessing.scale(X)\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()   # perform PCA and get 30 new coordinates that we call principal components (PCs)\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "\n",
    "print(f\"pca.explained_variance_ratio_ = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, 2)\n",
    "print(f\"per_var = {per_var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"PC{i}\" for i in range(1,31)]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.bar(x = range(1, 31), height = per_var, tick_label = labels)\n",
    "plt.xlabel('Principal Component', fontsize = 15)\n",
    "plt.ylabel('Percentage of Variation', fontsize = 15)\n",
    "plt.title('Scree Plot', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows how each PC can be obtained using the original coordinates\n",
    "\n",
    "# PC1 = 0.219 * mean radius - 0.234 * mean texture ......\n",
    "\n",
    "labels = [f\"PC{i}\" for i in range(1, 31)]\n",
    "\n",
    "pd.DataFrame(pca.components_, columns = labels, index = cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca.transform(scaled_X)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_data, columns = labels)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def color(label):\n",
    "    if label == 0:\n",
    "        return \"magenta\"\n",
    "    else:\n",
    "        return \"lightseagreen\"\n",
    "\n",
    "colors = [color(label) for label in cancer_df[\"target\"]]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2, c = colors)\n",
    "plt.xlabel(\"PC1\", fontsize = 15)\n",
    "plt.ylabel(\"PC2\", fontsize = 15)\n",
    "plt.title(\"Two-component PCA\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
